import pandas as pd
import matplotlib.pyplot as plt
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
import os

# --- Configuration ---
# A list of the CSV files generated by the overnight runs.
# The script will look for these specific filenames.
#RESULT_FILES = {
#    "Llama 3.1 8B": "experiment_A_overnight_results_filtered_llama31.csv",
#    "Qwen2 7B": "experiment_A_overnight_results_filtered_qwen2.csv",
#}
RESULT_FILES = {
    "Qwen2 7B MPS": "experiment_A_overnight_results_one_loop_filtered_qwen2.csv",
    "Qwen2 7B T4": "experiment_A_overnight_results_Qwen2_filtered_colab.csv",
}

#OUTPUT_PERFORMANCE_CHART = "comparative_performance.png"
#OUTPUT_ACCURACY_CHART = "comparative_accuracy.png"
OUTPUT_PERFORMANCE_CHART = "comparative_performance_MPSvsT4.png"
OUTPUT_ACCURACY_CHART = "comparative_accuracy_MPSvsT4.png"

console = Console()

def load_and_merge_data():
    """Loads all individual result CSVs and merges them into a single dataframe."""
    all_dfs = []
    console.print("--- Loading and merging result files ---")
    for model_name, file_path in RESULT_FILES.items():
        if not os.path.exists(file_path):
            console.print(f"[bold red]Error: Result file not found: '{file_path}'[/bold red]")
            console.print(f"[yellow]Skipping analysis for {model_name}. Please ensure the file exists.[/yellow]")
            continue
        
        df = pd.read_csv(file_path)
        df['model_name'] = model_name
        all_dfs.append(df)
        console.print(f"  [green]Loaded {file_path} for {model_name}[/green]")
    
    if not all_dfs:
        return None
        
    return pd.concat(all_dfs, ignore_index=True)

def generate_graphs(df: pd.DataFrame):
    """Generates two comparative graphs: one for performance and one for accuracy."""
    console.print("\n--- Generating comparative analysis graphs ---")
    plt.style.use('seaborn-v0_8-whitegrid')

    # --- 1. Performance Graph (Response Time vs. Context Size) ---
    perf_summary = df.groupby(['model_name', 'context_size'])['response_time'].mean().reset_index()
    fig1, ax1 = plt.subplots(figsize=(12, 8))
    for name, group in perf_summary.groupby('model_name'):
        ax1.plot(group['context_size'], group['response_time'], marker='o', linestyle='-', label=name)
    
    ax1.set_title('Performance Impact of Context Size Across Models', fontsize=16, pad=20)
    ax1.set_xlabel('Context Size (Tokens)', fontsize=12)
    ax1.set_ylabel('Average Response Time (seconds)', fontsize=12)
    ax1.set_xticks(sorted(df['context_size'].unique()))
    ax1.legend(title='Model')
    ax1.grid(True, which='both', linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    plt.savefig(OUTPUT_PERFORMANCE_CHART)
    console.print(f"[green]Performance graph saved to [cyan]{OUTPUT_PERFORMANCE_CHART}[/cyan][/green]")

    # --- 2. Accuracy Graph (Accuracy vs. Context Size) ---
    acc_summary = df.groupby(['model_name', 'context_size'])['accuracy'].mean().reset_index()
    fig2, ax2 = plt.subplots(figsize=(12, 8))
    for name, group in acc_summary.groupby('model_name'):
        ax2.plot(group['context_size'], group['accuracy'], marker='o', linestyle='--', label=name)

    ax2.set_title('Accuracy "Distraction Effect" Across Models', fontsize=16, pad=20)
    ax2.set_xlabel('Context Size (Tokens)', fontsize=12)
    ax2.set_ylabel('Average Accuracy on General Knowledge Questions', fontsize=12)
    ax2.set_xticks(sorted(df['context_size'].unique()))
    ax2.legend(title='Model')
    ax2.grid(True, which='both', linestyle='--', alpha=0.7)
    ax2.set_ylim(0, 1.05) # Accuracy is between 0 and 1
    ax2.yaxis.set_major_formatter(plt.FuncFormatter('{:.0%}'.format))


    plt.tight_layout()
    plt.savefig(OUTPUT_ACCURACY_CHART)
    console.print(f"[green]Accuracy graph saved to [cyan]{OUTPUT_ACCURACY_CHART}[/cyan][/green]")


def main():
    """Main function to run the comparative analysis."""
    console.print(Panel("[bold cyan]Comparative Analysis of Large Context Performance[/bold cyan]", border_style="green"))
    
    merged_df = load_and_merge_data()
    
    if merged_df is None or merged_df.empty:
        console.print("[bold red]No data to analyze. Exiting.[/bold red]")
        return

    # --- Print Summary Table ---
    summary = merged_df.groupby(['model_name', 'context_size']).agg(
        avg_response_time=('response_time', 'mean'),
        avg_accuracy=('accuracy', 'mean')
    ).reset_index()
    
    console.print("\n--- Overall Performance Summary ---")
    
    # Pivot for better readability in the console
    pivot_summary = summary.pivot(index='context_size', columns='model_name', values=['avg_response_time', 'avg_accuracy'])
    console.print(pivot_summary.round(4))

    # --- Generate Graphs ---
    generate_graphs(merged_df)
    
    console.print(Panel("[bold magenta]Comparative analysis complete![/bold magenta]"))

if __name__ == "__main__":
    main()
